---
title: "SamplingMethods"
author: "Halil Bilgin"
date: "10 Nisan 2017"
output:
  word_document: 
    fig_height: 4
    fig_width: 6
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pander)
library(dplyr)
library(tidyr)
library(reshape2, quietly = T)
library(plyr, quietly = T)
library(scales, quietly = T)

source('../autoload.R', chdir=T)
algorithms <- c('lda')

algorithms <- loadClassifiers(algorithms)

repetitions <- 50

cat('Constant Seed:', seed, '\n')
library(doMC)
registerDoMC(cores = 4)

samplingMethods <- c('smote', 'none', 'up', 'down')

set.seed(seed)
formulas <- c(
  featureSelection$aic(c(featureSelection$fm(db,T), featureSelection$fm(db,F)), scaledDb)
  )
names(formulas) <- 1:length(formulas)

print(lapply(formulas, function(fm) {
  paste(format(fm), collapse='')
}))
set.seed(seed)

result <- data.frame(
  method=character(),
  Accuracy=numeric(),
  AccuracyNull=numeric(),
  Sensitivity=numeric(),
  Specificity=numeric(),
  Kappa=numeric()
)

```

## Features Comparison

```r repetitions``` Different training-test split was used to measure performance of each formula-sampling method combination. Table below represents confidence intervals of the different scores.

```{r trainingModels, echo=FALSE, include=FALSE}
sapply(names(formulas), function(fmNo) {
  fm <- formulas[fmNo]
  for(i in 1:length(algorithms)) {
    lapply(samplingMethods, function(nm) {
      
      sapply(names(algorithms), function(algorithm) {
        classifier <- get('algorithms')[[algorithm]]
        for(i in 1:repetitions) {
          set.seed(seed+i*100+1000)
          tt <- manipulations$trainTest(get('db'), p=0.75)
          scaledTt <- manipulations$scaledTrainTest(tt$train, tt$test)
          
          control <- trainControl(method="none", allowParallel = T, sampling=if(nm=='none') NULL else nm)
          
          fit <- classifier(scaledTt$train, get('fm')[[1]], method=algorithm,
                            trControl=control)
          confMatrix <- confusionMatrix(predict(fit, newdata=scaledTt$test),
                                        scaledTt$test$gfrloss2)
          
          assign("result", rbind(get('result'), data.frame(
            method=paste(algorithm, nm, 'formula', get('fmNo')),
            Accuracy=confMatrix$overall['Accuracy'],
            AccuracyNull=confMatrix$overall['AccuracyNull'],
            Sensitivity=confMatrix$byClass['Sensitivity'],
            Specificity=confMatrix$byClass['Specificity'],
            Kappa=confMatrix$overall['Kappa']
          )), envir = .GlobalEnv)
          
        }
    })
    })
    
  }
})

  
```


```{r kappavals, echo=FALSE}
result2 <- result[0, ]

c<-sapply(levels(result$method), function(method) {
  result <- get('result')
  x<- lapply(result[result$method == method,], function(x){
    if(! is.numeric(x)) {
      return(x[[1]])
    }
    tTest <- t.test(x)
    sMean <- round(tTest$estimate[[1]],3)
    sError <- round(tTest$estimate[[1]] - tTest$conf.int[1], 3)
    
    c(as.character(paste(sMean, '\u00B1', sError)))
    
  })
  
  levels(x$method) <- levels(result$method)
  x <- data.frame(x, stringsAsFactors=FALSE)
  assign('result2', rbind(get('result2'), x), envir = .GlobalEnv)

})

knitr::kable(result2, row.names = F)
  
```
